{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from string import digits\n",
    "import sys\n",
    "import html\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('SampleInput.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "l=list()\n",
    "for c,i in enumerate(df.Title):\n",
    "    if (str(i).find('Ticket Assigned to')!=-1) or (str(i).find('Ticket Updated')!=-1) or (str(i).find('Ticket Marked Overdue')!=-1) or (str(i).find('Status Changed')!=-1):\n",
    "        l.append(c)\n",
    "        #df_new=df.drop(df.index[c])\n",
    "    if str(i).find('nan')!=-1:\n",
    "        l.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new=df.drop(df.index[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I placed few question marks in the word document in last few headings and named the changed file with addition of QM\n",
    "text = textract.process(\"SampleDocuments/SampleInputDoc1-FAQs_QM.docx\").decode(\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_input_doc1=text.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faq=pd.DataFrame(columns=['Problem','Solution'])\n",
    "c=-1\n",
    "for i in list_input_doc1:\n",
    "    try:\n",
    "        if i.find('?')!=-1:\n",
    "            c+=1\n",
    "            df_faq.loc[c,'Problem']=i\n",
    "        else:\n",
    "            df_faq.loc[c,'Solution']=str(df_faq.loc[c,'Solution'])+ '\\n'+i\n",
    "            df_faq.loc[c,'Solution']=df_faq.loc[c,'Solution'].replace('nan\\n','')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input_doc2 = textract.process(\"SampleDocuments/SampleInputDoc2-percent.docx\").decode(\"utf-8\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_input_doc2=text_input_doc2.split('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input_doc_2=pd.DataFrame(columns=['Problem','Solution'])\n",
    "c=-1        \n",
    "for i in list_input_doc2:\n",
    "    try:\n",
    "        if i.find('%')!=-1:\n",
    "            c+=1\n",
    "            df_input_doc_2.loc[c,'Problem']=i.replace('%','')\n",
    "        else:\n",
    "            df_input_doc_2.loc[c,'Solution']=str(df_input_doc_2.loc[c,'Solution'])+ '\\n'+i\n",
    "            df_input_doc_2.loc[c,'Solution']=df_input_doc_2.loc[c,'Solution'].replace('nan\\n','')\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = docx.Document('SampleDocuments/SampleInputDoc3-Hardware Problems.docx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "c=-1\n",
    "df_hardware=pd.DataFrame(columns=['Main_Problem','Symptom','Diagnosis'])\n",
    "for i in range(len(doc.paragraphs)):\n",
    "    if(str(doc.paragraphs[i].style).find('Heading 4')!=-1):\n",
    "        Main_Problem=doc.paragraphs[i].text\n",
    "    elif(str(doc.paragraphs[i].style).find('Heading 5')!=-1):\n",
    "        if(doc.paragraphs[i].text=='Symptom'):\n",
    "            c+=1\n",
    "            df_hardware.loc[c,'Main_Problem']=Main_Problem\n",
    "            df_hardware.loc[c,'Symptom']=doc.paragraphs[i+1].text\n",
    "        if(doc.paragraphs[i].text=='Diagnosis'):\n",
    "            j=i\n",
    "            try:\n",
    "                while((doc.paragraphs[j+1].text!='Symptom') | (str(doc.paragraphs[j+1].style).find('Heading 4')!=-1)):\n",
    "                    df_hardware.loc[c,'Diagnosis']=str(df_hardware.loc[c,'Diagnosis'])+'\\n'+doc.paragraphs[j].text\n",
    "                    df_hardware.loc[c,'Diagnosis']=df_hardware.loc[c,'Diagnosis'].replace('nan\\nDiagnosis','')\n",
    "                    j+=1\n",
    "            except:\n",
    "                pass\n",
    "            i=j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hardware.loc[10,'Diagnosis']='''Programs that require network drives to run or operate properly: SIS, HR, FRS, PROD – ALPHA, Network Shares, and some school applications. You would also need a network connection to print to the network laser or color laser printers within CCRI.'''\n",
    "df_hardware.loc[8,'Diagnosis']='Reboot the computer and see if that corrects the problem. If not check to see if there is insufficient memory.'\n",
    "df_hardware.loc[11,'Diagnosis']='SampleDocuments/pic_references/11.png'\n",
    "df_hardware.loc[12,'Diagnosis']='SampleDocuments/pic_references/12.png'\n",
    "df_hardware.loc[15,'Diagnosis']='SampleDocuments/pic_references/15.png'\n",
    "df_hardware.loc[18,'Diagnosis']='SampleDocuments/pic_references/18.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaning(text, escape_list=[], stop=[]):\n",
    "    l=[]\n",
    "    \"\"\"\n",
    "    Text cleaning function:\n",
    "        Input: \n",
    "            -text: a string variable, the text to be cleaned\n",
    "            -escape_list : words not to transform by the cleaning process (only lowcase transformation is needed)  \n",
    "            -stop : custom stopwords\n",
    "        Output:\n",
    "            -text cleaned and stemmed           \n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" Get stop word list from package\"\"\"\n",
    "    StopWords = list(set(stopwords.words('english')))\n",
    "    custom_stop = StopWords + stop\n",
    "    \n",
    "    \"\"\" Step 1: Parse html entities\"\"\"\n",
    "    text = html.unescape(text)\n",
    "    text=text.replace('\\n',' ').replace('\\t',' ').replace('’','')\n",
    "    \n",
    "    tokenz=nltk.word_tokenize(text)\n",
    "       \n",
    "    \"\"\" Step 2: Remove stop words \"\"\"\n",
    "    tokenz=([token for token in tokenz if token not in custom_stop]) \n",
    "\n",
    "    \"\"\" Step 4: Remove digits from tokens\"\"\"\n",
    "    remove_digits = str.maketrans('', '', digits)\n",
    "    tokenz=[token.translate(remove_digits)  if token not in  escape_list else token for token in tokenz   ]\n",
    "    \n",
    "    \"\"\" Step 5: Lowcase the text\"\"\"\n",
    "    tokenz=([token.lower() for token in tokenz])\n",
    "    \"\"\" Step 6: Stemming the text\"\"\"\n",
    "    tokenz=[EnglishStemmer().stem(token) if token not in escape_list else token for token in tokenz ]\n",
    "\n",
    "    return ' '.join(tokenz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new_4join=df_new.loc[:,['Title','Resolution']]\n",
    "df_new_4join.columns=['Problem','Solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_faq.columns=['Problem','Solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hardware_4join=df_hardware.loc[:,['Symptom','Diagnosis']]\n",
    "df_hardware_4join.columns=['Problem','Solution']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df_new_4join,df_faq, df_hardware_4join,df_input_doc_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df_all=df_all.copy()\n",
    "clean_df_all['Problem']=df_all['Problem'].apply(text_cleaning)\n",
    "clean_df_all.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "clean_df_all.to_pickle(\"clean_df_all.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
